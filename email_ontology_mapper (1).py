#!/usr/bin/env python3
"""
HVDC Email to Ontology Mapper
MACHO-GPT v3.4-mini - Email Intelligence System

ê¸°ëŠ¥:
- ì´ë©”ì¼ ë°ì´í„°ë¥¼ RDF/TTL ì˜¨í†¨ë¡œì§€ë¡œ ë³€í™˜
- í”„ë¡œì íŠ¸ ì‚¬ì´íŠ¸, LPO, ë²¤ë”, ì¼€ì´ìŠ¤ ìë™ ì¶”ì¶œ
- HVDC ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ í™•ì¥
"""

import pandas as pd
import re
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Set
import json


class HVDCEmailOntologyMapper:
    """ì´ë©”ì¼ ë°ì´í„°ë¥¼ HVDC ì˜¨í†¨ë¡œì§€ë¡œ ë§¤í•‘"""
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = None
        self.ontology_data = []
        self.statistics = defaultdict(int)
        
        # íŒ¨í„´ ì •ì˜
        self.patterns = {
            'site': r'\b(DAS|AGI|MIR|MIRFA|GHALLAN)\b',
            'lpo': r'LPO[-\s]?(\d+)',
            'case': r'HVDC[-\s]([A-Z]+)[-\s]([A-Z]+)[-\s]([A-Z0-9\-]+)',
            'po': r'P[OJ]C?[-\s]LPO[-\s](\d+)',
            'invoice': r'(BAMF\d+|INV[-\s]?\d+)',
            'container': r'(CONTAINER|CONT|CNT)[-\s]?([A-Z0-9]+)',
            'delivery': r'(DELIVERY|DELIVER)',
            'urgent': r'(URGENT|IMMEDIATE|ASAP|CRITICAL)',
        }
        
        # ë²¤ë” ë„ë©”ì¸ ë§¤í•‘
        self.vendor_domains = {}
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        self.namespaces = {
            'ex': 'http://samsung.com/project-logistics#',
            'email': 'http://samsung.com/hvdc/email#',
            'vendor': 'http://samsung.com/hvdc/vendor#',
            'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
            'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',
            'xsd': 'http://www.w3.org/2001/XMLSchema#',
        }
    
    def load_data(self):
        """CSV íŒŒì¼ ë¡œë“œ"""
        print(f"ğŸ“§ ì´ë©”ì¼ ë°ì´í„° ë¡œë“œ ì¤‘: {self.csv_path}")
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8-sig')
            print(f"   âœ… {len(self.df):,}ê°œ ì´ë©”ì¼ ë¡œë“œ ì™„ë£Œ")
            print(f"   ğŸ“… ê¸°ê°„: {self.df['received_time'].min()} ~ {self.df['received_time'].max()}")
            return True
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_metadata(self, row: pd.Series) -> Dict:
        """ì´ë©”ì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        subject = str(row['subject'])
        sender_email = str(row['sender_email'])
        
        metadata = {
            'email_id': f"email_{row.name}",
            'subject': subject,
            'sender_name': row['sender_name'],
            'sender_email': sender_email,
            'received_time': row['received_time'],
            'has_attachments': bool(row['has_attachments']),
            'folder': row['folder'],
        }
        
        # ì‚¬ì´íŠ¸ ì¶”ì¶œ
        site_match = re.search(self.patterns['site'], subject, re.IGNORECASE)
        if site_match:
            metadata['site'] = site_match.group(1).upper()
            self.statistics['emails_with_site'] += 1
        
        # LPO ë²ˆí˜¸ ì¶”ì¶œ
        lpo_matches = re.findall(self.patterns['lpo'], subject, re.IGNORECASE)
        if lpo_matches:
            metadata['lpo_numbers'] = [f"LPO-{lpo}" for lpo in lpo_matches]
            self.statistics['emails_with_lpo'] += 1
        
        # ì¼€ì´ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ
        case_match = re.search(self.patterns['case'], subject, re.IGNORECASE)
        if case_match:
            metadata['case_number'] = f"HVDC-{'-'.join(case_match.groups())}"
            self.statistics['emails_with_case'] += 1
        
        # ì¸ë³´ì´ìŠ¤ ì¶”ì¶œ
        invoice_match = re.search(self.patterns['invoice'], subject, re.IGNORECASE)
        if invoice_match:
            metadata['invoice_number'] = invoice_match.group(1)
            self.statistics['emails_with_invoice'] += 1
        
        # ì»¨í…Œì´ë„ˆ ì¶”ì¶œ
        container_match = re.search(self.patterns['container'], subject, re.IGNORECASE)
        if container_match:
            metadata['has_container'] = True
            self.statistics['emails_with_container'] += 1
        
        # ë°°ì†¡ ê´€ë ¨
        if re.search(self.patterns['delivery'], subject, re.IGNORECASE):
            metadata['is_delivery'] = True
            self.statistics['delivery_emails'] += 1
        
        # ê¸´ê¸‰ ì—¬ë¶€
        if re.search(self.patterns['urgent'], subject, re.IGNORECASE):
            metadata['is_urgent'] = True
            self.statistics['urgent_emails'] += 1
        
        # ë²¤ë” ë„ë©”ì¸
        domain = sender_email.split('@')[-1] if '@' in sender_email else 'unknown'
        metadata['vendor_domain'] = domain
        self.vendor_domains[domain] = self.vendor_domains.get(domain, 0) + 1
        
        return metadata
    
    def classify_email_type(self, metadata: Dict) -> str:
        """ì´ë©”ì¼ íƒ€ì… ë¶„ë¥˜"""
        subject = metadata['subject'].lower()
        
        if 're:' in subject or 'fwd:' in subject:
            return 'Reply'
        elif 'lpo' in subject or 'purchase' in subject:
            return 'PurchaseOrder'
        elif 'delivery' in subject or 'deliver' in subject:
            return 'DeliveryNotification'
        elif 'invoice' in subject:
            return 'Invoice'
        elif 'quotation' in subject or 'rfq' in subject:
            return 'Quotation'
        elif 'approval' in subject or 'approve' in subject:
            return 'Approval'
        elif 'urgent' in subject or 'immediate' in subject:
            return 'Urgent'
        else:
            return 'General'
    
    def map_to_ontology(self):
        """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤í–‰"""
        print("\nğŸ”„ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹œì‘...")
        
        for idx, row in self.df.iterrows():
            if idx % 1000 == 0:
                print(f"   ì§„í–‰ë¥ : {idx}/{len(self.df)} ({idx/len(self.df)*100:.1f}%)")
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.extract_metadata(row)
            
            # ì´ë©”ì¼ íƒ€ì… ë¶„ë¥˜
            email_type = self.classify_email_type(metadata)
            metadata['email_type'] = email_type
            
            # ì˜¨í†¨ë¡œì§€ ë°ì´í„° ìƒì„±
            self.ontology_data.append(metadata)
            self.statistics['total_processed'] += 1
        
        print(f"   âœ… {self.statistics['total_processed']:,}ê°œ ì´ë©”ì¼ ë§¤í•‘ ì™„ë£Œ")
    
    def generate_ttl(self, output_file: str = "hvdc_emails_ontology.ttl"):
        """TTL íŒŒì¼ ìƒì„±"""
        print(f"\nğŸ“ TTL ì˜¨í†¨ë¡œì§€ ìƒì„± ì¤‘: {output_file}")
        
        ttl_lines = []
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„ ì–¸
        for prefix, uri in self.namespaces.items():
            ttl_lines.append(f"@prefix {prefix}: <{uri}> .")
        ttl_lines.append("")
        
        # ì˜¨í†¨ë¡œì§€ í—¤ë”
        ttl_lines.append("# =================================================================")
        ttl_lines.append("# HVDC Email Ontology - September 2025")
        ttl_lines.append("# MACHO-GPT v3.4-mini | Samsung C&T Ã— ADNOCÂ·DSV")
        ttl_lines.append(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        ttl_lines.append(f"# Total Emails: {len(self.ontology_data):,}")
        ttl_lines.append("# =================================================================")
        ttl_lines.append("")
        
        # í´ë˜ìŠ¤ ì •ì˜
        ttl_lines.append("# Email Classes")
        ttl_lines.append("email:Email a rdfs:Class ;")
        ttl_lines.append('    rdfs:label "ì´ë©”ì¼"@ko .')
        ttl_lines.append("")
        
        ttl_lines.append("email:PurchaseOrder a rdfs:Class ;")
        ttl_lines.append('    rdfs:subClassOf email:Email ;')
        ttl_lines.append('    rdfs:label "êµ¬ë§¤ ì£¼ë¬¸"@ko .')
        ttl_lines.append("")
        
        ttl_lines.append("email:DeliveryNotification a rdfs:Class ;")
        ttl_lines.append('    rdfs:subClassOf email:Email ;')
        ttl_lines.append('    rdfs:label "ë°°ì†¡ í†µì§€"@ko .')
        ttl_lines.append("")
        
        ttl_lines.append("vendor:Vendor a rdfs:Class ;")
        ttl_lines.append('    rdfs:label "ë²¤ë”"@ko .')
        ttl_lines.append("")
        
        # ì†ì„± ì •ì˜
        ttl_lines.append("# Email Properties")
        ttl_lines.append("email:hasSubject a rdf:Property ;")
        ttl_lines.append('    rdfs:label "ì œëª©"@ko ;')
        ttl_lines.append('    rdfs:domain email:Email ;')
        ttl_lines.append('    rdfs:range xsd:string .')
        ttl_lines.append("")
        
        ttl_lines.append("email:hasSender a rdf:Property ;")
        ttl_lines.append('    rdfs:label "ë°œì‹ ì"@ko ;')
        ttl_lines.append('    rdfs:domain email:Email ;')
        ttl_lines.append('    rdfs:range vendor:Vendor .')
        ttl_lines.append("")
        
        ttl_lines.append("email:receivedTime a rdf:Property ;")
        ttl_lines.append('    rdfs:label "ìˆ˜ì‹  ì‹œê°„"@ko ;')
        ttl_lines.append('    rdfs:domain email:Email ;')
        ttl_lines.append('    rdfs:range xsd:dateTime .')
        ttl_lines.append("")
        
        ttl_lines.append("email:relatedToSite a rdf:Property ;")
        ttl_lines.append('    rdfs:label "ê´€ë ¨ ì‚¬ì´íŠ¸"@ko ;')
        ttl_lines.append('    rdfs:domain email:Email ;')
        ttl_lines.append('    rdfs:range ex:Site .')
        ttl_lines.append("")
        
        ttl_lines.append("email:relatedToLPO a rdf:Property ;")
        ttl_lines.append('    rdfs:label "ê´€ë ¨ LPO"@ko ;')
        ttl_lines.append('    rdfs:domain email:Email ;')
        ttl_lines.append('    rdfs:range xsd:string .')
        ttl_lines.append("")
        
        # ì´ë©”ì¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìƒ˜í”Œ 500ê°œ)
        ttl_lines.append("# Email Instances (Sample 500)")
        for i, data in enumerate(self.ontology_data[:500]):
            email_id = data['email_id']
            email_type = data['email_type']
            
            ttl_lines.append(f"email:{email_id} a email:{email_type} ;")
            ttl_lines.append(f'    email:hasSubject """{data["subject"]}""" ;')
            ttl_lines.append(f'    email:hasSender vendor:{self._sanitize_id(data["vendor_domain"])} ;')
            ttl_lines.append(f'    email:receivedTime "{data["received_time"]}"^^xsd:dateTime ;')
            ttl_lines.append(f'    email:hasAttachments "{str(data["has_attachments"]).lower()}"^^xsd:boolean ;')
            
            if 'site' in data:
                ttl_lines.append(f'    email:relatedToSite ex:Site_{data["site"]} ;')
            
            if 'lpo_numbers' in data:
                for lpo in data['lpo_numbers']:
                    ttl_lines.append(f'    email:relatedToLPO "{lpo}" ;')
            
            if 'case_number' in data:
                ttl_lines.append(f'    email:relatedToCase ex:Case_{self._sanitize_id(data["case_number"])} ;')
            
            if data.get('is_urgent'):
                ttl_lines.append(f'    email:isUrgent "true"^^xsd:boolean ;')
            
            # ë§ˆì§€ë§‰ ì¤„ ì²˜ë¦¬
            ttl_lines[-1] = ttl_lines[-1].rstrip(' ;') + ' .'
            ttl_lines.append("")
        
        # ë²¤ë” ì¸ìŠ¤í„´ìŠ¤ (ìƒìœ„ 20ê°œ)
        ttl_lines.append("# Top Vendors")
        top_vendors = sorted(self.vendor_domains.items(), key=lambda x: x[1], reverse=True)[:20]
        for domain, count in top_vendors:
            vendor_id = self._sanitize_id(domain)
            ttl_lines.append(f"vendor:{vendor_id} a vendor:Vendor ;")
            ttl_lines.append(f'    rdfs:label "{domain}" ;')
            ttl_lines.append(f'    vendor:emailCount {count} .')
            ttl_lines.append("")
        
        # íŒŒì¼ ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(ttl_lines))
        
        print(f"   âœ… TTL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ğŸ“¦ ì´ {len(ttl_lines):,}ì¤„ ìƒì„±")
    
    def _sanitize_id(self, text: str) -> str:
        """IDë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë¬¸ìì—´ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì¹˜í™˜
        text = re.sub(r'[^\w\-]', '_', text)
        text = re.sub(r'_+', '_', text)
        text = text.strip('_')
        return text
    
    def generate_analysis_report(self, output_file: str = "hvdc_email_analysis.md"):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\nğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")
        
        md = []
        md.append("# HVDC ì´ë©”ì¼ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë¶„ì„ ë¦¬í¬íŠ¸")
        md.append("\n**MACHO-GPT v3.4-mini** | Samsung C&T Logistics Ã— ADNOCÂ·DSV")
        md.append(f"\n**ìƒì„±ì¼:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md.append(f"\n---\n")
        
        # ì „ì²´ í†µê³„
        md.append("## ğŸ“Š ì „ì²´ í†µê³„")
        md.append(f"- **ì´ ì´ë©”ì¼ ìˆ˜:** {self.statistics['total_processed']:,}ê°œ")
        md.append(f"- **ì‚¬ì´íŠ¸ ê´€ë ¨:** {self.statistics['emails_with_site']:,}ê°œ ({self.statistics['emails_with_site']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **LPO í¬í•¨:** {self.statistics['emails_with_lpo']:,}ê°œ ({self.statistics['emails_with_lpo']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **ì¼€ì´ìŠ¤ ë²ˆí˜¸ í¬í•¨:** {self.statistics['emails_with_case']:,}ê°œ ({self.statistics['emails_with_case']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **ì¸ë³´ì´ìŠ¤ ê´€ë ¨:** {self.statistics['emails_with_invoice']:,}ê°œ ({self.statistics['emails_with_invoice']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **ì»¨í…Œì´ë„ˆ ê´€ë ¨:** {self.statistics['emails_with_container']:,}ê°œ ({self.statistics['emails_with_container']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **ë°°ì†¡ ê´€ë ¨:** {self.statistics['delivery_emails']:,}ê°œ ({self.statistics['delivery_emails']/self.statistics['total_processed']*100:.1f}%)")
        md.append(f"- **ê¸´ê¸‰ ì´ë©”ì¼:** {self.statistics['urgent_emails']:,}ê°œ ({self.statistics['urgent_emails']/self.statistics['total_processed']*100:.1f}%)")
        
        # ì´ë©”ì¼ íƒ€ì… ë¶„í¬
        md.append("\n## ğŸ“§ ì´ë©”ì¼ íƒ€ì… ë¶„í¬")
        email_types = defaultdict(int)
        for data in self.ontology_data:
            email_types[data['email_type']] += 1
        
        for email_type, count in sorted(email_types.items(), key=lambda x: x[1], reverse=True):
            percentage = count / self.statistics['total_processed'] * 100
            md.append(f"- **{email_type}:** {count:,}ê°œ ({percentage:.1f}%)")
        
        # ì‚¬ì´íŠ¸ë³„ ë¶„í¬
        md.append("\n## ğŸ—ï¸ ì‚¬ì´íŠ¸ë³„ ì´ë©”ì¼ ë¶„í¬")
        sites = defaultdict(int)
        for data in self.ontology_data:
            if 'site' in data:
                sites[data['site']] += 1
        
        for site, count in sorted(sites.items(), key=lambda x: x[1], reverse=True):
            md.append(f"- **{site}:** {count:,}ê°œ")
        
        # ìƒìœ„ ë²¤ë”
        md.append("\n## ğŸ¢ ìƒìœ„ ë²¤ë” (ì´ë©”ì¼ ë°œì†¡ ê¸°ì¤€)")
        top_vendors = sorted(self.vendor_domains.items(), key=lambda x: x[1], reverse=True)[:20]
        for i, (domain, count) in enumerate(top_vendors, 1):
            md.append(f"{i}. **{domain}:** {count:,}ê°œ")
        
        # ì²¨ë¶€íŒŒì¼ í†µê³„
        md.append("\n## ğŸ“ ì²¨ë¶€íŒŒì¼ í†µê³„")
        with_attachments = sum(1 for d in self.ontology_data if d['has_attachments'])
        md.append(f"- **ì²¨ë¶€íŒŒì¼ ìˆìŒ:** {with_attachments:,}ê°œ ({with_attachments/len(self.ontology_data)*100:.1f}%)")
        md.append(f"- **ì²¨ë¶€íŒŒì¼ ì—†ìŒ:** {len(self.ontology_data)-with_attachments:,}ê°œ")
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        md.append("\n## â° ì‹œê°„ëŒ€ë³„ ì´ë©”ì¼ ë¶„í¬")
        hours = defaultdict(int)
        for data in self.ontology_data:
            try:
                dt = datetime.fromisoformat(data['received_time'].replace('T', ' '))
                hours[dt.hour] += 1
            except:
                pass
        
        for hour in sorted(hours.keys()):
            count = hours[hour]
            bar = 'â–ˆ' * (count // 50)
            md.append(f"- **{hour:02d}ì‹œ:** {count:,}ê°œ {bar}")
        
        md.append("\n---")
        md.append("\n*Generated by HVDC Email Ontology Mapper*")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md))
        
        print(f"   âœ… ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_file}")
    
    def export_to_json(self, output_file: str = "hvdc_emails_structured.json"):
        """êµ¬ì¡°í™”ëœ JSON ë‚´ë³´ë‚´ê¸°"""
        print(f"\nğŸ’¾ JSON ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {output_file}")
        
        export_data = {
            'metadata': {
                'total_emails': len(self.ontology_data),
                'generated_at': datetime.now().isoformat(),
                'source_file': self.csv_path,
                'statistics': dict(self.statistics)
            },
            'emails': self.ontology_data[:1000],  # ìƒ˜í”Œ 1000ê°œ
            'vendors': [
                {'domain': domain, 'email_count': count}
                for domain, count in sorted(self.vendor_domains.items(), key=lambda x: x[1], reverse=True)[:50]
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… JSON ì €ì¥ ì™„ë£Œ: {output_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*70)
    print("ğŸš€ HVDC Email Ontology Mapper")
    print("   MACHO-GPT v3.4-mini | Samsung C&T Logistics")
    print("="*70)
    
    # ë§¤í¼ ì´ˆê¸°í™”
    mapper = HVDCEmailOntologyMapper('emails_sep_full.csv')
    
    # ë°ì´í„° ë¡œë“œ
    if not mapper.load_data():
        return
    
    # ì˜¨í†¨ë¡œì§€ ë§¤í•‘
    mapper.map_to_ontology()
    
    # TTL ìƒì„±
    mapper.generate_ttl('hvdc_emails_ontology.ttl')
    
    # ë¶„ì„ ë¦¬í¬íŠ¸
    mapper.generate_analysis_report('hvdc_email_analysis.md')
    
    # JSON ë‚´ë³´ë‚´ê¸°
    mapper.export_to_json('hvdc_emails_structured.json')
    
    print("\n" + "="*70)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*70)
    print("\nğŸ“‚ ìƒì„±ëœ íŒŒì¼:")
    print("   â€¢ hvdc_emails_ontology.ttl - RDF/TTL ì˜¨í†¨ë¡œì§€ (500ê°œ ìƒ˜í”Œ)")
    print("   â€¢ hvdc_email_analysis.md - ë¶„ì„ ë¦¬í¬íŠ¸")
    print("   â€¢ hvdc_emails_structured.json - êµ¬ì¡°í™”ëœ JSON (1000ê°œ ìƒ˜í”Œ)")
    print("\nğŸ”— ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. TTL íŒŒì¼ì„ ê¸°ì¡´ HVDC ì˜¨í†¨ë¡œì§€ì™€ í†µí•©")
    print("   2. SPARQL ì¿¼ë¦¬ë¡œ ì´ë©”ì¼-í”„ë¡œì íŠ¸ ê´€ê³„ ë¶„ì„")
    print("   3. ì‹œê°í™” ë„êµ¬ë¡œ ì´ë©”ì¼ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±")


if __name__ == "__main__":
    main()



